# ReplicatedConfigMap
![CI](https://github.com/abatilo/replicatedconfigmap/workflows/CI/badge.svg)

`ReplicatedConfigMap` is an Operator that comes bundled with a CRD such that you
can created a `ReplicatedConfigMap` resource, and the data that's placed into a
`ReplicatedConfigMap` will be duplicated to namespaces with a specific
annotation.

`ReplicatedConfigMap` is a simple project to learn the basics around developing
an Operator for the Kubernetes ecosystem.

## Requirements / Goals
There is a small number of requirements that the `ReplicatedConfigMap` needs to
address.

1. `ReplicatedConfigMap` is implemented via a CRD. The provided functionality
   could have been implemented with just a controller and labels or annotations
   on a `ConfigMap`, but we explicitly wanted to create a CRD.
2. The `ReplicatedConfigMap` controller is to only pay attention to namespaces
   with a specific annotation.
3. The `ReplicatedConfigMap` is to be built using `kubebuilder` as opposed to
   other Operator frameworks.
4. The `ReplicatedConfigMap` needs to immediately react to new namespaces that
   are created with the appropriate annotations.

## Points of interest

### Testing

Integration tests are ran on every push via [GitHub Actions](https://github.com/abatilo/replicatedconfigmap/actions).

These go beyond the scope of just unit tests, as we actually run Kubernetes in
the GitHub Action execution environment using the
[kind](https://github.com/kubernetes-sigs/kind) project.

### Tooling

All commands and development were done via a very small utility container found
in the `./hack` folder. From the root of the repository, you can run `make -f
hack/Makefile tools` which will build a container and create a shell for you
with `kubebuilder`, `kubectl`, and `go` installed. This means that there's a
standardized and portable development environment that can be used for
developing the controller itself. The only things missing are running kind
within this environment so that you don't need to setup minikube or a dev
cluster or anything of the sort and to use something like
[reflex](https://github.com/cespare/reflex) or
[CompileDaemon](https://github.com/githubnemo/CompileDaemon) for hot reloaded
application development.

### Deployment

Given more time, I'd consider packaging up the controller binary and hosting it
on DockerHub or another registry. For the purposes of this little project, the
`make run &` in the GitHub Action suffices.

## Questions

### Do I need to run the controller in each namespace that I want it to listen to?

No, the `ReplicatedConfigMap` controller was built using `kubebuilder`'s
cluster wide mode. This means that when you create a `ReplicatedConfigMap`, it
actually doesn't live in any namespace at all.

Allowing for cross namespace owner references without being a non-namespaced
resource is [explicitly not
allowed](https://github.com/kubernetes-sigs/controller-runtime/pull/675) by the
`controller-runtime` project. We've configured the CRD to be generated by
kubebuilder as a non-namespaced resource via the scope parameter documented
[here](https://book.kubebuilder.io/reference/markers/crd.html).

### What's the behavior of removing managed ConfigMaps?

ConfigMaps that were created by the `ReplicatedConfigMap` controller that are
deleted manually will get instantly re-created by the controller.

### What's the behavior of removing the annotation set on a namespace?

The ConfigMaps that were created will stay there. There was no requirement to
reconcile deletion of ConfigMaps when a namespace is no longer being watched.

### What's the behavior of removing the source `ReplicatedConfigMap`

All children of the source `ReplicatedConfigMap` CRD will get deleted via
Kubernetes' ownership model.

### Why aren't there more unit tests?

Mostly due to a lack of time. The application implementation is very small
because we explicitly decided to not invest a ton of time in reconciliation
logic for a bunch of edge cases.

The value in this controller lives in its behaviors in an actual Kubernetes
cluster, thus we decided that focusing efforts on integration testing and
figuring out how to automate that became much more compelling.
